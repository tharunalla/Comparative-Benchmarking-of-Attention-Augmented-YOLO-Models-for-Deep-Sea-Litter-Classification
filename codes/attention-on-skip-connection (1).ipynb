{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11528710,"sourceType":"datasetVersion","datasetId":7230842},{"sourceId":11528863,"sourceType":"datasetVersion","datasetId":7230952},{"sourceId":367767,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":304739,"modelId":325189},{"sourceId":367777,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":304743,"modelId":325192},{"sourceId":367790,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":304751,"modelId":325199}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%cd /kaggle/input/attention-skip3/pytorch/default/1/ultralytics-topic2-updated-2/ultralytics-topic2-20250427T161602Z-001/ultralytics-topic2/ultralytics-main\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T14:54:37.093732Z","iopub.execute_input":"2025-05-01T14:54:37.094236Z","iopub.status.idle":"2025-05-01T14:54:37.110112Z","shell.execute_reply.started":"2025-05-01T14:54:37.094194Z","shell.execute_reply":"2025-05-01T14:54:37.109252Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/attention-skip3/pytorch/default/1/ultralytics-topic2-updated-2/ultralytics-topic2-20250427T161602Z-001/ultralytics-topic2/ultralytics-main\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!cp -r /kaggle/input/attention-skip3/pytorch/default/1/ultralytics-topic2-updated-2/ultralytics-topic2-20250427T161602Z-001/ultralytics-topic2/ultralytics-main /kaggle/working/ultralytics-main\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T14:54:39.753843Z","iopub.execute_input":"2025-05-01T14:54:39.754534Z","iopub.status.idle":"2025-05-01T14:54:44.827563Z","shell.execute_reply.started":"2025-05-01T14:54:39.754508Z","shell.execute_reply":"2025-05-01T14:54:44.826618Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"%cd /kaggle/working/ultralytics-main\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T14:54:52.891363Z","iopub.execute_input":"2025-05-01T14:54:52.891913Z","iopub.status.idle":"2025-05-01T14:54:52.897860Z","shell.execute_reply.started":"2025-05-01T14:54:52.891882Z","shell.execute_reply":"2025-05-01T14:54:52.897259Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/ultralytics-main\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install -e .\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T14:54:58.389383Z","iopub.execute_input":"2025-05-01T14:54:58.389634Z","iopub.status.idle":"2025-05-01T14:56:19.639729Z","shell.execute_reply.started":"2025-05-01T14:54:58.389615Z","shell.execute_reply":"2025-05-01T14:56:19.638933Z"}},"outputs":[{"name":"stdout","text":"Obtaining file:///kaggle/working/ultralytics-main\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: numpy<=2.1.1,>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.117) (1.26.4)\nRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.117) (3.7.5)\nRequirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.117) (4.11.0.86)\nRequirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.117) (11.1.0)\nRequirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.117) (6.0.2)\nRequirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.117) (2.32.3)\nRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.117) (1.15.2)\nRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.117) (2.5.1+cu124)\nRequirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.117) (0.20.1+cu124)\nRequirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.117) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.117) (7.0.0)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.117) (9.0.0)\nRequirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.117) (2.2.3)\nRequirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.117) (0.12.2)\nCollecting ultralytics-thop>=2.0.0 (from ultralytics==8.3.117)\n  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.117) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.117) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.117) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.117) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.117) (24.2)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.117) (3.2.1)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.117) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics==8.3.117) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics==8.3.117) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics==8.3.117) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics==8.3.117) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics==8.3.117) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics==8.3.117) (2.4.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics==8.3.117) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics==8.3.117) (2025.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics==8.3.117) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics==8.3.117) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics==8.3.117) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics==8.3.117) (2025.1.31)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.117) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.117) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.117) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.117) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.117) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.117) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.117) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.117) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics==8.3.117)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics==8.3.117)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics==8.3.117)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics==8.3.117)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics==8.3.117)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics==8.3.117)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.117) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.117) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics==8.3.117)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.117) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.117) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics==8.3.117) (1.3.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics==8.3.117) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics==8.3.117) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<=2.1.1,>=1.23.0->ultralytics==8.3.117) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<=2.1.1,>=1.23.0->ultralytics==8.3.117) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<=2.1.1,>=1.23.0->ultralytics==8.3.117) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<=2.1.1,>=1.23.0->ultralytics==8.3.117) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<=2.1.1,>=1.23.0->ultralytics==8.3.117) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\nBuilding wheels for collected packages: ultralytics\n  Building editable for ultralytics (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for ultralytics: filename=ultralytics-8.3.117-0.editable-py3-none-any.whl size=23119 sha256=2745ef06571f179e55d2cd86c1f8faf002e855bca9b9a3c3d1505e3235a360c9\n  Stored in directory: /tmp/pip-ephem-wheel-cache-99wj_xt3/wheels/64/39/24/dd15456076dfff1f1b91b914383a398341547c8e3da2c6b087\nSuccessfully built ultralytics\nInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ultralytics-8.3.117 ultralytics-thop-2.0.14\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from ultralytics import YOLO\n\nmodel = YOLO(\"/kaggle/input/attention-skip3/pytorch/default/1/ultralytics-topic2-updated-2/ultralytics-topic2-20250427T161602Z-001/ultralytics-topic2/ultralytics-main/ultralytics/cfg/models/11/yolo11.yaml\").load(\"yolo11s.pt\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T14:57:32.987493Z","iopub.execute_input":"2025-05-01T14:57:32.988185Z","iopub.status.idle":"2025-05-01T14:57:38.609726Z","shell.execute_reply.started":"2025-05-01T14:57:32.988154Z","shell.execute_reply":"2025-05-01T14:57:38.609106Z"}},"outputs":[{"name":"stdout","text":"Creating new Ultralytics Settings v0.0.6 file ✅ \nView Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\nWARNING ⚠️ no model scale passed. Assuming scale='n'.\nBefore processing - CBAM args: [128, 128, 16, 7]\nCBAM constructed with c1=128, ratio=16, kernel_size=7\nCBAM initialized with c1=128, ratio=16, kernel_size=7, extra_args=(), extra_kwargs={}\nDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s.pt to 'yolo11s.pt'...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 18.4M/18.4M [00:00<00:00, 153MB/s]","output_type":"stream"},{"name":"stdout","text":"Transferred 20/502 items from pretrained weights\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"\nmodel.train(\n    data=\"/kaggle/input/pathed/data.yaml\",\n    epochs=300,\n    batch=16,\n    imgsz=416,\n    save=True,\n    patience=300,\n    val=True\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T14:57:56.755126Z","iopub.execute_input":"2025-05-01T14:57:56.755508Z"}},"outputs":[{"name":"stdout","text":"New https://pypi.org/project/ultralytics/8.3.122 available 😃 Update with 'pip install -U ultralytics'\nUltralytics 8.3.117 🚀 Python-3.11.11 torch-2.5.1+cu124 CUDA:0 (Tesla T4, 15095MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=/kaggle/input/attention-skip3/pytorch/default/1/ultralytics-topic2-updated-2/ultralytics-topic2-20250427T161602Z-001/ultralytics-topic2/ultralytics-main/ultralytics/cfg/models/11/yolo11.yaml, data=/kaggle/input/pathed/data.yaml, epochs=300, time=None, patience=300, batch=16, imgsz=416, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=yolo11s.pt, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\nDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 755k/755k [00:00<00:00, 18.0MB/s]\n","output_type":"stream"},{"name":"stdout","text":"WARNING ⚠️ no model scale passed. Assuming scale='n'.\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \nBefore processing - CBAM args: [128, 128, 16, 7]\nCBAM constructed with c1=128, ratio=16, kernel_size=7\nCBAM initialized with c1=128, ratio=16, kernel_size=7, extra_args=(), extra_kwargs={}\n  6                  -1  1      2146  ultralytics.nn.modules.cbam.CustomCBAM       [128, 128, 16, 7]             \n  7                  -2  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n  8                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  9                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n 10                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 11                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n 12                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 13             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 14                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n 15                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 16             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 17                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n 18                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 19            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 20                  -1  1    119488  ultralytics.nn.modules.block.C3k2            [448, 128, 1, False]          \n 21                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 22            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 23                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n 24        [16, 19, 22]  1   1252252  ultralytics.nn.modules.head.Detect           [4, [256, 448, 384]]          \nYOLO11 summary: 188 layers, 3,446,334 parameters, 3,446,318 gradients, 11.0 GFLOPs\n\nTransferred 502/502 items from pretrained weights\nFreezing layer 'model.24.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\nDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5.35M/5.35M [00:00<00:00, 70.2MB/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.2±0.2 ms, read: 8.0±4.3 MB/s, size: 41.1 KB)\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/marine/train/labels... 2436 images, 420 backgrounds, 0 corrupt: 100%|██████████| 2436/2436 [00:09<00:00, 251.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"WARNING ⚠️ \u001b[34m\u001b[1mtrain: \u001b[0mCache directory /kaggle/input/marine/train is not writeable, cache not saved.\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 6.2±1.7 MB/s, size: 32.1 KB)\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/marine/valid/labels... 266 images, 42 backgrounds, 0 corrupt: 100%|██████████| 266/266 [00:01<00:00, 246.21it/s]","output_type":"stream"},{"name":"stdout","text":"WARNING ⚠️ \u001b[34m\u001b[1mval: \u001b[0mCache directory /kaggle/input/marine/valid is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Plotting labels to runs/detect/train/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 81 weight(decay=0.0), 91 weight(decay=0.0005), 87 bias(decay=0.0)\nImage sizes 416 train, 416 val\nUsing 2 dataloader workers\nLogging results to \u001b[1mruns/detect/train\u001b[0m\nStarting training for 300 epochs...\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n      1/300      1.46G      3.554      4.665      4.229         19        416: 100%|██████████| 153/153 [00:21<00:00,  7.17it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:02<00:00,  3.14it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295    0.00177      0.256    0.00233   0.000886\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n      2/300      1.73G      3.512      4.522       4.08          6        416: 100%|██████████| 153/153 [00:18<00:00,  8.07it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.08it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295    0.00133      0.325    0.00464    0.00169\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n      3/300      1.73G      3.083      4.304      3.619          2        416: 100%|██████████| 153/153 [00:18<00:00,  8.19it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.47it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295   0.000418     0.0959   0.000624   0.000181\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n      4/300      1.73G       2.73      4.069      3.055          9        416: 100%|██████████| 153/153 [00:18<00:00,  8.28it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.35it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.505     0.0134    0.00448    0.00147\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n      5/300      1.73G      2.603      3.922      2.866          4        416: 100%|██████████| 153/153 [00:18<00:00,  8.18it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.39it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295    0.00557      0.368     0.0144     0.0042\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n      6/300      1.73G      2.535      3.812      2.727          5        416: 100%|██████████| 153/153 [00:18<00:00,  8.09it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.90it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.285      0.029     0.0347     0.0106\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n      7/300      1.73G      2.472      3.671      2.641          8        416: 100%|██████████| 153/153 [00:18<00:00,  8.16it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.37it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.294      0.129     0.0508     0.0179\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n      8/300      1.73G      2.359      3.518       2.52          7        416: 100%|██████████| 153/153 [00:18<00:00,  8.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.87it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.559      0.054      0.066     0.0233\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n      9/300      1.73G      2.264      3.394      2.465          5        416: 100%|██████████| 153/153 [00:18<00:00,  8.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.80it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295     0.0865      0.107     0.0786     0.0296\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     10/300      1.73G      2.219      3.286      2.406          9        416: 100%|██████████| 153/153 [00:18<00:00,  8.24it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.07it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.699     0.0793     0.0995     0.0428\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     11/300      1.73G      2.144      3.162       2.34          8        416: 100%|██████████| 153/153 [00:18<00:00,  8.17it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.98it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.152      0.194      0.122     0.0483\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     12/300      1.73G      2.104      3.096      2.295          7        416: 100%|██████████| 153/153 [00:18<00:00,  8.16it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.65it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.187      0.163      0.121     0.0506\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     13/300      1.73G      2.065      3.064      2.271         10        416: 100%|██████████| 153/153 [00:18<00:00,  8.19it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.19it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295       0.21      0.202      0.142     0.0539\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     14/300      1.73G      2.035      3.008      2.254          8        416: 100%|██████████| 153/153 [00:18<00:00,  8.07it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.09it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.187      0.226       0.13     0.0531\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     15/300      1.73G      2.014      2.978      2.214          8        416: 100%|██████████| 153/153 [00:18<00:00,  8.20it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.21it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.252      0.173      0.153     0.0672\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     16/300      1.73G      2.006      2.913      2.181          9        416: 100%|██████████| 153/153 [00:18<00:00,  8.07it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.45it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295       0.15      0.221      0.154     0.0619\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     17/300      1.73G      1.966      2.881      2.161          8        416: 100%|██████████| 153/153 [00:18<00:00,  8.16it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.28it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.186      0.296       0.15     0.0604\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     18/300      1.73G      1.944      2.874      2.152         17        416: 100%|██████████| 153/153 [00:18<00:00,  8.14it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.94it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.229      0.254      0.176     0.0792\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     19/300      1.73G      1.954      2.852      2.145          6        416: 100%|██████████| 153/153 [00:18<00:00,  8.13it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.47it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.247      0.291       0.21     0.0864\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     20/300      1.73G      1.932      2.819       2.13          7        416: 100%|██████████| 153/153 [00:18<00:00,  8.15it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.20it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.235      0.277      0.194     0.0898\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     21/300      1.73G      1.929       2.81      2.119          4        416: 100%|██████████| 153/153 [00:18<00:00,  8.18it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.16it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295       0.24      0.304      0.214     0.0948\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     22/300      1.73G      1.922      2.764        2.1          3        416: 100%|██████████| 153/153 [00:18<00:00,  8.13it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.19it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.241      0.265       0.21     0.0878\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     23/300      1.73G      1.924      2.777      2.104          6        416: 100%|██████████| 153/153 [00:18<00:00,  8.21it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.13it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295       0.25      0.311      0.222      0.102\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     24/300      1.73G      1.883       2.76      2.095          3        416: 100%|██████████| 153/153 [00:18<00:00,  8.15it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.17it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.192      0.336      0.187     0.0827\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     25/300      1.73G      1.876      2.722      2.076          8        416: 100%|██████████| 153/153 [00:18<00:00,  8.16it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.16it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295       0.27      0.308      0.241      0.108\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     26/300      1.73G      1.867      2.694      2.057          7        416: 100%|██████████| 153/153 [00:18<00:00,  8.12it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.23it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.321      0.343      0.279      0.126\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     27/300      1.73G      1.846      2.656      2.044          9        416: 100%|██████████| 153/153 [00:18<00:00,  8.18it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.06it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.288      0.287       0.26      0.111\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     28/300      1.73G       1.84      2.633      2.038          7        416: 100%|██████████| 153/153 [00:18<00:00,  8.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.38it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.326      0.356       0.27      0.114\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     29/300      1.73G      1.827      2.623      2.029          8        416: 100%|██████████| 153/153 [00:18<00:00,  8.23it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.15it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.292      0.354      0.262      0.115\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     30/300      1.73G      1.841      2.628      2.015         12        416: 100%|██████████| 153/153 [00:18<00:00,  8.09it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.99it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.286      0.346      0.262      0.109\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     31/300      1.73G      1.842      2.646      2.021          6        416: 100%|██████████| 153/153 [00:18<00:00,  8.23it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.39it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.304      0.338       0.25       0.11\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     32/300      1.73G      1.824      2.601      2.012         17        416: 100%|██████████| 153/153 [00:18<00:00,  8.16it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.25it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.324       0.27      0.251      0.113\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     33/300      1.73G      1.817      2.612      2.002          9        416: 100%|██████████| 153/153 [00:18<00:00,  8.12it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.20it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.297      0.356      0.271      0.125\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     34/300      1.73G      1.816      2.557      2.006         15        416: 100%|██████████| 153/153 [00:18<00:00,  8.17it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.32it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.291      0.326      0.282      0.124\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     35/300      1.73G      1.799       2.54      1.997          5        416: 100%|██████████| 153/153 [00:18<00:00,  8.15it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.348      0.342      0.313      0.136\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     36/300      1.73G      1.802       2.57      1.994         12        416: 100%|██████████| 153/153 [00:18<00:00,  8.14it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.45it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.268      0.367       0.27      0.123\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     37/300      1.73G       1.82       2.54      2.005          6        416: 100%|██████████| 153/153 [00:18<00:00,  8.15it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.34it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295       0.34      0.344      0.285      0.129\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     38/300      1.73G      1.805      2.519      1.987          4        416: 100%|██████████| 153/153 [00:18<00:00,  8.12it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.22it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295       0.29       0.36      0.283      0.131\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     39/300      1.73G      1.782      2.478       1.96          7        416: 100%|██████████| 153/153 [00:18<00:00,  8.14it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.38it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.305      0.372      0.277      0.139\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     40/300      1.73G      1.767      2.483      1.963          6        416: 100%|██████████| 153/153 [00:18<00:00,  8.17it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.76it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.281      0.354      0.282      0.125\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     41/300      1.73G      1.765      2.453       1.96          5        416: 100%|██████████| 153/153 [00:18<00:00,  8.14it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.41it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.332      0.362      0.314      0.148\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     42/300      1.73G       1.76      2.422       1.93          8        416: 100%|██████████| 153/153 [00:18<00:00,  8.18it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.31it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295       0.37      0.327      0.324      0.154\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     43/300      1.73G      1.745      2.454      1.947         10        416: 100%|██████████| 153/153 [00:18<00:00,  8.21it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.38it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.383      0.341      0.334      0.152\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     44/300      1.73G       1.75      2.428      1.944          6        416: 100%|██████████| 153/153 [00:18<00:00,  8.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.05it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.328      0.379      0.304      0.151\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     45/300      1.73G      1.737      2.432      1.929          5        416: 100%|██████████| 153/153 [00:18<00:00,  8.19it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.35it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.315      0.416      0.322       0.14\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     46/300      1.73G      1.746      2.451      1.957          3        416: 100%|██████████| 153/153 [00:18<00:00,  8.09it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.44it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.382      0.404      0.336      0.168\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     47/300      1.73G      1.751      2.447      1.953          7        416: 100%|██████████| 153/153 [00:18<00:00,  8.12it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.24it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.369      0.326      0.357      0.168\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     48/300      1.73G      1.726      2.389      1.916          8        416: 100%|██████████| 153/153 [00:18<00:00,  8.21it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.305      0.353      0.296       0.14\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     49/300      1.73G      1.745      2.406      1.922         15        416: 100%|██████████| 153/153 [00:18<00:00,  8.08it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.20it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.378       0.38      0.335      0.162\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     50/300      1.73G       1.74      2.403      1.935          4        416: 100%|██████████| 153/153 [00:18<00:00,  8.16it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.10it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.388      0.415      0.346       0.17\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     51/300      1.73G      1.705      2.381      1.905          6        416: 100%|██████████| 153/153 [00:18<00:00,  8.25it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.39it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295       0.32      0.345      0.319      0.152\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     52/300      1.73G      1.717      2.356      1.901          6        416: 100%|██████████| 153/153 [00:18<00:00,  8.12it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.28it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.337      0.354      0.322      0.154\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     53/300      1.73G      1.738      2.347      1.917          7        416: 100%|██████████| 153/153 [00:18<00:00,  8.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.31it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.392      0.396      0.377      0.196\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     54/300      1.73G      1.724       2.33       1.91         11        416: 100%|██████████| 153/153 [00:18<00:00,  8.08it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.99it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.374      0.376       0.32      0.159\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     55/300      1.73G      1.716      2.354      1.909          6        416: 100%|██████████| 153/153 [00:18<00:00,  8.16it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.95it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.337      0.422      0.348       0.18\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     56/300      1.73G       1.72      2.319      1.891         11        416: 100%|██████████| 153/153 [00:18<00:00,  8.18it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.11it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295       0.49      0.324      0.354      0.169\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     57/300      1.73G      1.713      2.297      1.892          7        416: 100%|██████████| 153/153 [00:18<00:00,  8.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.02it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.443      0.363      0.355      0.173\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     58/300      1.73G      1.681      2.265      1.878         12        416: 100%|██████████| 153/153 [00:18<00:00,  8.12it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.01it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.402      0.376      0.365      0.182\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     59/300      1.73G      1.691      2.268      1.885          9        416: 100%|██████████| 153/153 [00:18<00:00,  8.17it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.41it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.457      0.381      0.371      0.177\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     60/300      1.73G      1.672      2.225      1.874         16        416: 100%|██████████| 153/153 [00:19<00:00,  8.05it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.13it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.429      0.326      0.351      0.179\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     61/300      1.73G      1.666      2.276      1.875          5        416: 100%|██████████| 153/153 [00:18<00:00,  8.26it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.94it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.385      0.385      0.348      0.174\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     62/300      1.73G      1.682      2.243      1.877          4        416: 100%|██████████| 153/153 [00:18<00:00,  8.15it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.24it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.435      0.435      0.402      0.209\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     63/300      1.73G      1.684      2.242      1.871          7        416: 100%|██████████| 153/153 [00:18<00:00,  8.14it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.40it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.414      0.399      0.398      0.201\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     64/300      1.73G      1.681      2.236      1.877          9        416: 100%|██████████| 153/153 [00:18<00:00,  8.18it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.45it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.455      0.409      0.414      0.211\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     65/300      1.73G      1.669      2.217      1.883          7        416: 100%|██████████| 153/153 [00:18<00:00,  8.08it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.56it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.391      0.441      0.394      0.182\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     66/300      1.73G      1.682      2.204      1.883         11        416: 100%|██████████| 153/153 [00:18<00:00,  8.09it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.90it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.479      0.415      0.401        0.2\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     67/300      1.73G      1.653      2.191      1.858          4        416: 100%|██████████| 153/153 [00:18<00:00,  8.21it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.32it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.486      0.406      0.405      0.207\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     68/300      1.73G      1.655      2.227      1.861          3        416: 100%|██████████| 153/153 [00:19<00:00,  8.04it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.43it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.428       0.41      0.386      0.188\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     69/300      1.73G      1.654      2.171      1.859          5        416: 100%|██████████| 153/153 [00:18<00:00,  8.14it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.67it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.429       0.38      0.383      0.197\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     70/300      1.73G      1.657       2.16      1.859         12        416: 100%|██████████| 153/153 [00:18<00:00,  8.15it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.423      0.441      0.413      0.207\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     71/300      1.73G      1.657      2.192      1.865         10        416: 100%|██████████| 153/153 [00:19<00:00,  8.02it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.24it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.404      0.387      0.374      0.189\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     72/300      1.73G      1.664      2.186      1.867          4        416: 100%|██████████| 153/153 [00:18<00:00,  8.16it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.49it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.468      0.381      0.406      0.197\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     73/300      1.73G      1.671      2.113      1.856         13        416: 100%|██████████| 153/153 [00:18<00:00,  8.13it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.22it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.434      0.414      0.391      0.198\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     74/300      1.73G       1.63      2.133      1.851          6        416: 100%|██████████| 153/153 [00:18<00:00,  8.12it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.36it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.406      0.436      0.405      0.203\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     75/300      1.73G      1.636      2.127      1.833          4        416: 100%|██████████| 153/153 [00:18<00:00,  8.19it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.32it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.536      0.406      0.431      0.205\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     76/300      1.73G      1.652      2.147      1.858          6        416: 100%|██████████| 153/153 [00:18<00:00,  8.13it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.05it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.497      0.401      0.416      0.207\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     77/300      1.73G      1.622       2.09      1.829          9        416: 100%|██████████| 153/153 [00:18<00:00,  8.13it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.26it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.473      0.404      0.399      0.192\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     78/300      1.73G      1.614      2.084      1.806         13        416: 100%|██████████| 153/153 [00:18<00:00,  8.20it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.52it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.457      0.418      0.415      0.208\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     79/300      1.73G      1.623       2.08      1.842         11        416: 100%|██████████| 153/153 [00:18<00:00,  8.15it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.03it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.379      0.448      0.373      0.193\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     80/300      1.73G      1.601      2.099      1.817          4        416: 100%|██████████| 153/153 [00:18<00:00,  8.19it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.64it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.451      0.463      0.456      0.223\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     81/300      1.73G      1.613      2.087      1.833         13        416: 100%|██████████| 153/153 [00:18<00:00,  8.17it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.49it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.411      0.471      0.422      0.213\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     82/300      1.73G      1.609      2.064      1.826          8        416: 100%|██████████| 153/153 [00:18<00:00,  8.13it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.30it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.526      0.384      0.437      0.222\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     83/300      1.73G      1.599      2.028      1.814          9        416: 100%|██████████| 153/153 [00:18<00:00,  8.15it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.35it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.495      0.448      0.444      0.234\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     84/300      1.73G        1.6      2.034      1.801          5        416: 100%|██████████| 153/153 [00:18<00:00,  8.15it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.28it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.407      0.441      0.428      0.217\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     85/300      1.73G      1.596      2.045      1.813          5        416: 100%|██████████| 153/153 [00:18<00:00,  8.14it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.42it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.526      0.439      0.458      0.235\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     86/300      1.73G      1.593      2.022      1.813          3        416: 100%|██████████| 153/153 [00:18<00:00,  8.14it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.32it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.498      0.426      0.429      0.223\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     87/300      1.73G      1.594      2.024      1.814          7        416: 100%|██████████| 153/153 [00:18<00:00,  8.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.41it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.468      0.445      0.445      0.229\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     88/300      1.73G      1.573      1.982      1.792          6        416: 100%|██████████| 153/153 [00:18<00:00,  8.13it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.71it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.434      0.481      0.439      0.225\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     89/300      1.73G      1.585       2.02      1.784         11        416: 100%|██████████| 153/153 [00:18<00:00,  8.19it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.01it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.477      0.491      0.445      0.237\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     90/300      1.73G      1.577       1.99      1.803          7        416: 100%|██████████| 153/153 [00:18<00:00,  8.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.93it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295       0.46       0.46      0.455      0.231\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     91/300      1.73G      1.575      1.988      1.801          4        416: 100%|██████████| 153/153 [00:18<00:00,  8.14it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.33it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.552      0.427      0.458      0.235\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     92/300      1.73G      1.575      1.996      1.801          3        416: 100%|██████████| 153/153 [00:18<00:00,  8.12it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.54it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.507      0.475      0.448       0.23\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     93/300      1.73G       1.59      1.993      1.802         11        416: 100%|██████████| 153/153 [00:18<00:00,  8.13it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.31it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295       0.52      0.467      0.459      0.231\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     94/300      1.73G      1.579      1.963      1.795          8        416: 100%|██████████| 153/153 [00:18<00:00,  8.16it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.35it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.523      0.396      0.443      0.228\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     95/300      1.73G      1.558      1.955      1.773          5        416: 100%|██████████| 153/153 [00:18<00:00,  8.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.34it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.509      0.445      0.448      0.233\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     96/300      1.73G      1.554      1.959      1.772          8        416: 100%|██████████| 153/153 [00:19<00:00,  8.01it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.25it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.486       0.49      0.455      0.239\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     97/300      1.73G      1.574      1.961      1.812          7        416: 100%|██████████| 153/153 [00:18<00:00,  8.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.37it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.533      0.453      0.451      0.231\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     98/300      1.73G      1.568      1.944      1.782          3        416: 100%|██████████| 153/153 [00:19<00:00,  7.99it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.90it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.476      0.484      0.457      0.235\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n     99/300      1.73G      1.565      1.938      1.792          9        416: 100%|██████████| 153/153 [00:18<00:00,  8.06it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.50it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.547       0.47      0.481      0.252\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    100/300      1.73G      1.563      1.967      1.794         12        416: 100%|██████████| 153/153 [00:18<00:00,  8.07it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.89it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295       0.54      0.476      0.482      0.243\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    101/300      1.73G      1.534      1.908       1.76         11        416: 100%|██████████| 153/153 [00:19<00:00,  7.95it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.07it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.575      0.419      0.471      0.248\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    102/300      1.73G      1.555      1.907      1.764          6        416: 100%|██████████| 153/153 [00:18<00:00,  8.08it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.01it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.486      0.486      0.472      0.243\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    103/300      1.73G      1.539      1.906       1.76          6        416: 100%|██████████| 153/153 [00:21<00:00,  7.22it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.35it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.528      0.483      0.469      0.243\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    104/300      1.73G      1.547      1.898      1.767         12        416: 100%|██████████| 153/153 [00:20<00:00,  7.54it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.00it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.475      0.506      0.462      0.245\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    105/300      1.73G      1.533      1.854      1.766          7        416: 100%|██████████| 153/153 [00:19<00:00,  7.75it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  5.40it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.523      0.452      0.466      0.242\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    106/300      1.73G      1.523      1.889      1.749          8        416: 100%|██████████| 153/153 [00:19<00:00,  7.75it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.35it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.494      0.435      0.454      0.236\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    107/300      1.73G      1.526      1.879      1.744         12        416: 100%|██████████| 153/153 [00:20<00:00,  7.64it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.95it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.543      0.491      0.491      0.253\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    108/300      1.73G      1.518      1.847      1.745          6        416: 100%|██████████| 153/153 [00:19<00:00,  7.66it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.33it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.531      0.486      0.474      0.245\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    109/300      1.73G      1.521      1.871      1.748          6        416: 100%|██████████| 153/153 [00:19<00:00,  7.66it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.02it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.508      0.478      0.486      0.248\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    110/300      1.73G      1.549      1.863      1.772         13        416: 100%|██████████| 153/153 [00:19<00:00,  7.96it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.07it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.539      0.445      0.484      0.245\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    111/300      1.73G      1.556      1.878      1.773          9        416: 100%|██████████| 153/153 [00:19<00:00,  7.74it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  5.47it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.555      0.434      0.466      0.249\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    112/300      1.73G      1.512      1.831      1.752          5        416: 100%|██████████| 153/153 [00:19<00:00,  7.76it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.39it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.505      0.447      0.462      0.245\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    113/300      1.73G       1.53      1.852      1.738         23        416: 100%|██████████| 153/153 [00:19<00:00,  7.69it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.88it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295       0.48      0.508      0.482      0.249\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    114/300      1.73G      1.504      1.819      1.735          4        416: 100%|██████████| 153/153 [00:19<00:00,  7.77it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  5.70it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.576      0.477      0.491      0.256\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    115/300      1.73G      1.523      1.814      1.735         10        416: 100%|██████████| 153/153 [00:19<00:00,  7.74it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.10it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.493      0.453      0.463      0.232\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    116/300      1.73G      1.498      1.821      1.728          8        416: 100%|██████████| 153/153 [00:19<00:00,  7.65it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.28it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.509      0.481      0.471      0.245\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    117/300      1.73G      1.515       1.78      1.731         13        416: 100%|██████████| 153/153 [00:18<00:00,  8.20it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.04it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.583      0.421      0.472      0.254\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    118/300      1.73G      1.511      1.814       1.73          5        416: 100%|██████████| 153/153 [00:19<00:00,  7.73it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.67it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.515      0.471      0.462      0.239\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    119/300      1.73G      1.501      1.792      1.734          6        416: 100%|██████████| 153/153 [00:19<00:00,  7.68it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.98it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.576      0.428      0.487      0.256\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    120/300      1.73G      1.495       1.79      1.722          2        416: 100%|██████████| 153/153 [00:19<00:00,  7.67it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.41it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.534      0.491      0.489      0.252\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    121/300      1.73G      1.501      1.811      1.728          3        416: 100%|██████████| 153/153 [00:19<00:00,  7.69it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.79it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.577      0.458      0.486      0.258\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    122/300      1.73G      1.487      1.772      1.723          5        416: 100%|██████████| 153/153 [00:19<00:00,  7.72it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.52it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.545      0.488       0.49      0.262\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    123/300      1.73G      1.491      1.772      1.724          6        416: 100%|██████████| 153/153 [00:19<00:00,  7.82it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.28it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295       0.49        0.5      0.488      0.249\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    124/300      1.73G      1.491      1.769       1.72         10        416: 100%|██████████| 153/153 [00:19<00:00,  7.99it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.19it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.515      0.503       0.48      0.241\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    125/300      1.73G      1.492      1.766      1.729         10        416: 100%|██████████| 153/153 [00:19<00:00,  7.78it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.84it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.557      0.461      0.496       0.26\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    126/300      1.73G      1.462      1.726      1.689         10        416: 100%|██████████| 153/153 [00:19<00:00,  7.70it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.82it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.503      0.524      0.498      0.263\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    127/300      1.73G      1.476      1.744      1.714          7        416: 100%|██████████| 153/153 [00:20<00:00,  7.64it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.44it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.552      0.477      0.481      0.256\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    128/300      1.73G      1.467      1.767      1.724         11        416: 100%|██████████| 153/153 [00:19<00:00,  7.70it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.22it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295       0.53      0.481      0.478      0.247\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    129/300      1.73G      1.458        1.7      1.688          7        416: 100%|██████████| 153/153 [00:19<00:00,  7.81it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.91it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.541      0.476      0.485      0.265\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    130/300      1.73G       1.48      1.745      1.718          2        416: 100%|██████████| 153/153 [00:19<00:00,  7.71it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.08it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295        0.6      0.438      0.482      0.261\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    131/300      1.73G       1.48      1.751      1.703          5        416: 100%|██████████| 153/153 [00:19<00:00,  7.73it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.62it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.556      0.495      0.505      0.264\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    132/300      1.73G      1.461      1.717      1.693          7        416: 100%|██████████| 153/153 [00:19<00:00,  7.80it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.66it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295       0.61      0.467      0.512      0.262\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    133/300      1.73G      1.486      1.703      1.694          4        416: 100%|██████████| 153/153 [00:19<00:00,  7.66it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.87it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.608       0.46      0.492      0.257\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    134/300      1.73G      1.473      1.739      1.697          8        416: 100%|██████████| 153/153 [00:20<00:00,  7.62it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.26it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.483      0.468      0.472      0.254\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    135/300      1.73G      1.463      1.738      1.705          3        416: 100%|██████████| 153/153 [00:19<00:00,  7.72it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.44it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.573      0.476      0.498      0.266\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    136/300      1.73G       1.44      1.693      1.683          7        416: 100%|██████████| 153/153 [00:20<00:00,  7.64it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.45it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.598      0.478       0.51      0.266\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    137/300      1.73G       1.46      1.692      1.693          7        416: 100%|██████████| 153/153 [00:20<00:00,  7.64it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.33it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.571      0.472      0.499      0.266\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    138/300      1.73G      1.449      1.685      1.685          9        416: 100%|██████████| 153/153 [00:19<00:00,  7.73it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.56it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.493       0.51        0.5      0.261\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    139/300      1.73G      1.458      1.675      1.699          4        416: 100%|██████████| 153/153 [00:19<00:00,  7.86it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.77it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295        0.6      0.495      0.513      0.264\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    140/300      1.73G      1.449      1.656      1.688          5        416: 100%|██████████| 153/153 [00:20<00:00,  7.61it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.24it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.647       0.48      0.504      0.261\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    141/300      1.73G      1.447      1.661      1.677          7        416: 100%|██████████| 153/153 [00:19<00:00,  7.67it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.08it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.656      0.442      0.504      0.264\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    142/300      1.73G      1.445      1.672      1.696         11        416: 100%|██████████| 153/153 [00:20<00:00,  7.61it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.05it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.542       0.48      0.492      0.256\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    143/300      1.73G      1.432      1.662      1.682          5        416: 100%|██████████| 153/153 [00:20<00:00,  7.64it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.64it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.544      0.512      0.508       0.27\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    144/300      1.73G      1.432      1.658       1.67          4        416: 100%|██████████| 153/153 [00:19<00:00,  7.69it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.29it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.596      0.468      0.497      0.267\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    145/300      1.73G      1.445      1.645      1.689          7        416: 100%|██████████| 153/153 [00:19<00:00,  7.74it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.42it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.578      0.483      0.518      0.281\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    146/300      1.73G      1.429      1.625      1.667          6        416: 100%|██████████| 153/153 [00:19<00:00,  7.92it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.86it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.628      0.471      0.517      0.278\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    147/300      1.73G      1.421      1.613      1.663          8        416: 100%|██████████| 153/153 [00:19<00:00,  7.81it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.44it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295       0.61      0.469       0.51      0.264\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    148/300      1.73G      1.416      1.582      1.662         11        416: 100%|██████████| 153/153 [00:19<00:00,  7.67it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.74it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.609      0.496       0.51      0.273\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    149/300      1.73G      1.433      1.614      1.662         20        416: 100%|██████████| 153/153 [00:19<00:00,  7.69it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.41it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.591      0.466      0.501      0.274\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    150/300      1.73G      1.432      1.624      1.674          7        416: 100%|██████████| 153/153 [00:19<00:00,  7.75it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.15it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.624       0.45      0.511      0.268\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    151/300      1.73G      1.419      1.604      1.646         11        416: 100%|██████████| 153/153 [00:19<00:00,  7.74it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.50it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.596      0.507      0.516      0.267\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    152/300      1.73G      1.421      1.589      1.651         13        416: 100%|██████████| 153/153 [00:19<00:00,  7.70it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.46it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.649       0.46      0.517      0.272\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    153/300      1.73G      1.424      1.622      1.666         13        416: 100%|██████████| 153/153 [00:18<00:00,  8.08it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.81it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.645      0.459      0.495      0.258\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    154/300      1.73G      1.422      1.617      1.664         14        416: 100%|██████████| 153/153 [00:19<00:00,  7.68it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.637      0.467      0.512      0.272\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    155/300      1.73G      1.393      1.578      1.642          7        416: 100%|██████████| 153/153 [00:19<00:00,  7.69it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.27it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.669      0.443      0.507      0.264\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    156/300      1.73G      1.409      1.587      1.652          4        416: 100%|██████████| 153/153 [00:19<00:00,  7.80it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.23it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.617      0.472       0.51      0.267\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    157/300      1.73G      1.402      1.592      1.651         17        416: 100%|██████████| 153/153 [00:19<00:00,  7.67it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.93it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.634      0.464      0.511      0.267\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    158/300      1.73G      1.408      1.564      1.653          4        416: 100%|██████████| 153/153 [00:19<00:00,  7.73it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.19it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.651      0.479      0.511      0.271\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    159/300      1.73G      1.408      1.587      1.663         11        416: 100%|██████████| 153/153 [00:19<00:00,  7.80it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.72it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.609      0.489      0.514      0.278\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    160/300      1.73G      1.397      1.534      1.651          9        416: 100%|██████████| 153/153 [00:19<00:00,  8.02it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.53it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.621       0.46      0.514      0.269\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    161/300      1.73G      1.396      1.585      1.655          6        416: 100%|██████████| 153/153 [00:19<00:00,  7.66it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.62it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.605      0.482      0.493      0.265\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    162/300      1.73G      1.394      1.551      1.634          6        416: 100%|██████████| 153/153 [00:19<00:00,  7.70it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.96it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.576      0.462      0.502      0.264\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    163/300      1.73G      1.388      1.528       1.64         10        416: 100%|██████████| 153/153 [00:19<00:00,  7.72it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.59it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.667      0.455      0.516       0.28\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    164/300      1.73G      1.389      1.549      1.635          4        416: 100%|██████████| 153/153 [00:20<00:00,  7.63it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.625      0.495      0.497      0.262\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    165/300      1.73G      1.391      1.562      1.635         11        416: 100%|██████████| 153/153 [00:19<00:00,  7.78it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.18it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.539        0.5      0.503      0.267\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    166/300      1.73G       1.39      1.545      1.642          8        416: 100%|██████████| 153/153 [00:19<00:00,  7.66it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.43it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.645      0.432      0.491      0.263\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    167/300      1.73G      1.383      1.532      1.648          5        416: 100%|██████████| 153/153 [00:19<00:00,  7.92it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.38it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.597      0.505      0.509      0.275\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    168/300      1.73G       1.39      1.537      1.642          2        416: 100%|██████████| 153/153 [00:19<00:00,  7.76it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.14it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.583      0.522       0.52      0.284\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    169/300      1.73G      1.393      1.545      1.642          5        416: 100%|██████████| 153/153 [00:19<00:00,  7.69it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.89it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.643      0.472      0.523      0.288\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    170/300      1.73G      1.362      1.525      1.637          7        416: 100%|██████████| 153/153 [00:20<00:00,  7.64it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.97it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.611      0.528      0.528      0.285\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    171/300      1.73G      1.355      1.499      1.627          5        416: 100%|██████████| 153/153 [00:19<00:00,  7.70it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.11it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295       0.65       0.48      0.509      0.277\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    172/300      1.73G      1.393      1.522       1.64         14        416: 100%|██████████| 153/153 [00:19<00:00,  7.68it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.29it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.667       0.51      0.535      0.284\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    173/300      1.73G      1.373      1.493      1.624          5        416: 100%|██████████| 153/153 [00:19<00:00,  7.71it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.29it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.615      0.508      0.523      0.279\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    174/300      1.73G      1.365      1.476      1.622          9        416: 100%|██████████| 153/153 [00:19<00:00,  7.81it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.43it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.641      0.506      0.537      0.288\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    175/300      1.73G      1.371      1.492      1.614          6        416: 100%|██████████| 153/153 [00:19<00:00,  7.83it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.10it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.632      0.524      0.536      0.291\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    176/300      1.73G      1.361      1.492      1.614          9        416: 100%|██████████| 153/153 [00:19<00:00,  7.70it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.84it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.646      0.497      0.531      0.287\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    177/300      1.73G      1.364      1.486      1.615          7        416: 100%|██████████| 153/153 [00:19<00:00,  7.75it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.96it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.613      0.518      0.537      0.291\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    178/300      1.73G      1.355       1.47      1.608          7        416: 100%|██████████| 153/153 [00:19<00:00,  7.69it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.75it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.597      0.506      0.529       0.29\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    179/300      1.73G      1.358      1.476      1.625         11        416: 100%|██████████| 153/153 [00:19<00:00,  7.74it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.63it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.593      0.507       0.53      0.286\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    180/300      1.73G      1.356      1.504      1.625         15        416: 100%|██████████| 153/153 [00:19<00:00,  7.73it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.07it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.577       0.53      0.529      0.285\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    181/300      1.73G      1.339      1.452      1.605         10        416: 100%|██████████| 153/153 [00:19<00:00,  7.81it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.27it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.632      0.477      0.529      0.285\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    182/300      1.73G      1.325       1.46      1.602          4        416: 100%|██████████| 153/153 [00:19<00:00,  7.84it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.17it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.557      0.518      0.529      0.292\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    183/300      1.73G      1.342      1.454      1.602          3        416: 100%|██████████| 153/153 [00:19<00:00,  7.81it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.95it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.629      0.505      0.534      0.297\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    184/300      1.73G      1.352      1.459      1.613          6        416: 100%|██████████| 153/153 [00:19<00:00,  7.73it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.09it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.565      0.533      0.521      0.286\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    185/300      1.73G      1.341      1.478      1.608          3        416: 100%|██████████| 153/153 [00:20<00:00,  7.65it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.47it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.612        0.5      0.524      0.285\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    186/300      1.73G      1.337       1.46      1.603          7        416: 100%|██████████| 153/153 [00:19<00:00,  7.68it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.85it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.613      0.496      0.526      0.284\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    187/300      1.73G      1.344      1.455      1.612          7        416: 100%|██████████| 153/153 [00:19<00:00,  7.66it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.09it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.628      0.528      0.529      0.284\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    188/300      1.73G      1.331      1.445      1.595         11        416: 100%|██████████| 153/153 [00:19<00:00,  7.75it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.76it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.594      0.551      0.539      0.289\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    189/300      1.73G      1.319      1.421      1.581          2        416: 100%|██████████| 153/153 [00:19<00:00,  8.02it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.04it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.612      0.529      0.545      0.296\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    190/300      1.73G      1.343      1.466      1.599          8        416: 100%|██████████| 153/153 [00:19<00:00,  7.66it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.00it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.602       0.54      0.551      0.299\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    191/300      1.73G      1.334      1.417      1.599          5        416: 100%|██████████| 153/153 [00:19<00:00,  7.76it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  5.74it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.599      0.527      0.552      0.308\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    192/300      1.73G      1.335      1.425      1.581          3        416: 100%|██████████| 153/153 [00:19<00:00,  7.66it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.69it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.584      0.521      0.545        0.3\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    193/300      1.73G      1.319      1.435       1.58          6        416: 100%|██████████| 153/153 [00:19<00:00,  7.71it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.20it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.597      0.518      0.541      0.297\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    194/300      1.73G       1.34      1.448      1.599         11        416: 100%|██████████| 153/153 [00:19<00:00,  7.67it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.64it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.624      0.516      0.549      0.298\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    195/300      1.73G      1.319      1.412      1.587         12        416: 100%|██████████| 153/153 [00:19<00:00,  7.82it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.88it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.683      0.481      0.545      0.294\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    196/300      1.73G        1.3      1.395       1.57          8        416: 100%|██████████| 153/153 [00:20<00:00,  7.52it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.13it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.604      0.508      0.542      0.294\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    197/300      1.73G      1.309      1.386      1.577          9        416: 100%|██████████| 153/153 [00:19<00:00,  7.75it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.68it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.549      0.565      0.542      0.291\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    198/300      1.73G       1.32      1.421      1.599          9        416: 100%|██████████| 153/153 [00:19<00:00,  7.71it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.12it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.655      0.485      0.531      0.283\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    199/300      1.73G      1.317      1.397      1.574          8        416: 100%|██████████| 153/153 [00:20<00:00,  7.65it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.46it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.626       0.51      0.531      0.283\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    200/300      1.73G      1.303      1.383      1.574          9        416: 100%|██████████| 153/153 [00:19<00:00,  7.83it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.10it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.691      0.472      0.537      0.289\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    201/300      1.73G      1.302      1.399      1.577          7        416: 100%|██████████| 153/153 [00:19<00:00,  7.68it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.38it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.722      0.495      0.534      0.296\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    202/300      1.73G      1.275      1.378      1.568         12        416: 100%|██████████| 153/153 [00:20<00:00,  7.65it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.23it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.685      0.485      0.527      0.294\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    203/300      1.73G      1.318      1.385      1.581          6        416: 100%|██████████| 153/153 [00:19<00:00,  7.89it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.41it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.622      0.518      0.534      0.297\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    204/300      1.73G      1.284      1.377      1.561          1        416: 100%|██████████| 153/153 [00:19<00:00,  7.84it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.89it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.661      0.502       0.54      0.296\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    205/300      1.73G      1.304       1.36       1.57         11        416: 100%|██████████| 153/153 [00:19<00:00,  7.69it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.35it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.638      0.529      0.538      0.293\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    206/300      1.73G      1.306      1.368       1.57          8        416: 100%|██████████| 153/153 [00:19<00:00,  7.73it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  5.99it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.651      0.507      0.534      0.294\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    207/300      1.73G      1.285      1.371      1.573          7        416: 100%|██████████| 153/153 [00:19<00:00,  7.81it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.17it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.643      0.524      0.541      0.297\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    208/300      1.73G      1.291      1.372      1.555         11        416: 100%|██████████| 153/153 [00:19<00:00,  7.71it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.85it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.642      0.532      0.538      0.287\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    209/300      1.73G      1.286      1.384      1.566          8        416: 100%|██████████| 153/153 [00:19<00:00,  7.77it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.29it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.643      0.522      0.537       0.29\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    210/300      1.73G      1.299      1.373      1.565         10        416: 100%|██████████| 153/153 [00:20<00:00,  7.63it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.49it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.659       0.51       0.54      0.287\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    211/300      1.73G      1.288      1.351      1.564          6        416: 100%|██████████| 153/153 [00:19<00:00,  7.92it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.27it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.653      0.521      0.546      0.296\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    212/300      1.73G      1.295      1.365       1.56          6        416: 100%|██████████| 153/153 [00:19<00:00,  7.78it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.01it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.669      0.512      0.547      0.299\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    213/300      1.73G      1.285      1.338      1.549          6        416: 100%|██████████| 153/153 [00:19<00:00,  7.70it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.13it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.655      0.523      0.546      0.299\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    214/300      1.73G      1.295      1.343      1.551         10        416: 100%|██████████| 153/153 [00:19<00:00,  7.71it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.11it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.658      0.513      0.543      0.299\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    215/300      1.73G      1.286       1.34       1.56          7        416: 100%|██████████| 153/153 [00:19<00:00,  7.74it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.57it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.662      0.509      0.543      0.293\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    216/300      1.73G       1.26      1.336      1.537         10        416: 100%|██████████| 153/153 [00:20<00:00,  7.60it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.45it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.646      0.507       0.54      0.288\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    217/300      1.73G      1.292      1.356       1.55          6        416: 100%|██████████| 153/153 [00:20<00:00,  7.62it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.39it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.617      0.542      0.543      0.289\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    218/300      1.73G      1.266      1.351      1.545          7        416: 100%|██████████| 153/153 [00:19<00:00,  8.03it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.99it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.593      0.528      0.537      0.286\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    219/300      1.73G      1.286      1.336      1.563          7        416: 100%|██████████| 153/153 [00:20<00:00,  7.62it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.74it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.624      0.512      0.536      0.285\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    220/300      1.73G       1.27      1.315      1.542          5        416: 100%|██████████| 153/153 [00:19<00:00,  7.68it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.87it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295       0.61      0.514      0.534      0.283\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    221/300      1.73G       1.28      1.324      1.538          5        416: 100%|██████████| 153/153 [00:19<00:00,  7.73it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.83it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.613      0.516      0.528      0.281\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    222/300      1.73G      1.291      1.341      1.568          8        416: 100%|██████████| 153/153 [00:20<00:00,  7.65it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.50it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.622      0.518      0.533      0.282\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    223/300      1.73G      1.261      1.306      1.538         18        416: 100%|██████████| 153/153 [00:19<00:00,  7.70it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.58it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.625      0.528      0.547      0.285\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    224/300      1.73G      1.285      1.322      1.556          6        416: 100%|██████████| 153/153 [00:19<00:00,  7.87it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.48it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.595      0.556      0.545      0.291\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    225/300      1.73G      1.258      1.308      1.535         11        416: 100%|██████████| 153/153 [00:18<00:00,  8.12it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.66it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.616      0.521      0.541      0.287\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    226/300      1.73G      1.284      1.341      1.557          5        416: 100%|██████████| 153/153 [00:20<00:00,  7.63it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.92it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.581      0.541      0.535      0.286\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    227/300      1.73G      1.244      1.304      1.521         11        416: 100%|██████████| 153/153 [00:19<00:00,  7.86it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.76it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.596      0.536      0.533      0.286\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    228/300      1.73G      1.248      1.304      1.533          4        416: 100%|██████████| 153/153 [00:19<00:00,  7.69it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.06it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.594      0.539      0.539       0.29\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    229/300      1.73G      1.261      1.318      1.539          5        416: 100%|██████████| 153/153 [00:20<00:00,  7.64it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.93it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.608      0.545       0.54      0.293\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    230/300      1.73G      1.264      1.311      1.534          7        416: 100%|██████████| 153/153 [00:19<00:00,  7.68it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.09it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.601       0.54      0.539      0.294\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    231/300      1.73G      1.236      1.287      1.525         14        416: 100%|██████████| 153/153 [00:19<00:00,  7.68it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.63it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.612      0.532      0.538      0.288\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    232/300      1.73G      1.263      1.286       1.54         14        416: 100%|██████████| 153/153 [00:19<00:00,  8.02it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.97it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.613      0.541      0.539      0.288\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    233/300      1.73G      1.235      1.284      1.517          9        416: 100%|██████████| 153/153 [00:19<00:00,  7.77it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.10it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.644      0.523      0.543      0.288\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    234/300      1.73G      1.242      1.263       1.52          8        416: 100%|██████████| 153/153 [00:20<00:00,  7.60it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.22it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.662      0.509      0.538      0.285\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    235/300      1.73G      1.249      1.273      1.523          8        416: 100%|██████████| 153/153 [00:19<00:00,  7.70it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  7.37it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295       0.64      0.525      0.538      0.286\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    236/300      1.73G      1.246      1.258      1.516          6        416: 100%|██████████| 153/153 [00:19<00:00,  7.81it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:01<00:00,  6.91it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        266        295      0.638      0.517      0.531      0.284\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/153 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:91.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    237/300      1.73G       1.31      1.298      1.596         33        416:   7%|▋         | 11/153 [00:01<00:17,  8.15it/s]","output_type":"stream"}],"execution_count":null}]}